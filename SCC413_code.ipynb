{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamilaSulaimanova/Text-Representations-for-Language-Identification/blob/main/SCC413_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlq3bU38Yq8O",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZg13JyNYsih"
      },
      "outputs": [],
      "source": [
        "!pip install hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGFIZM6YoeA"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import gc\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from itertools import islice\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# ML Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    f1_score\n",
        ")\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    logging as hf_logging\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Env setup\n",
        "# Memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# Ignore warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Set plots style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Hugging face setup\n",
        "# Enable faster downloads\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "# Get token\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "# Reduce transformers verbosity\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "# Dataset configs\n",
        "LANGUAGES = [\"ky\", \"kk\", \"ru\"]\n",
        "DATASET_NAME = \"oscar-corpus/OSCAR-2201\"\n",
        "N_SAMPLES = {\"ky\": 15000, \"kk\": 15000, \"ru\": 30000}\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Subword Tokenizer Configs\n",
        "# Common starting vocabluary size\n",
        "VOCAB_SIZE = 30000\n",
        "# Mininmun number of token appearance\n",
        "MIN_FREQUENCY = 5\n",
        "# Folder and prefix to store tokenizer files\n",
        "SUBWORD_MODEL_DIR = \"subword_tokenizer\"\n",
        "SUBWORD_MODEL_PREFIX = \"ky_kk_ru_bpe\"\n",
        "\n",
        "# Pre-trained Embeddings Configs\n",
        "EMBEDDING_MODEL_NAME = \"xlm-roberta-base\"\n",
        "EMBEDDING_BATCH_SIZE = 64\n",
        "\n",
        "# NLLB-based Code Switching Generation Configs\n",
        "NLLB_MODEL = \"facebook/nllb-200-distilled-600M\"\n",
        "# NLLB specific language codes\n",
        "NLLB_LANG_LABELS = {\"kk\": \"kaz_Cyrl\", \"ky\": \"kir_Cyrl\", \"ru\": \"rus_Cyrl\"}\n",
        "BASE_LANGS = [\"ky\", \"kk\"]\n",
        "MIX_LANG = \"ru\"\n",
        "# Words swap probabilities, mixing intensity\n",
        "WORD_SWAP_PROBS = [0.05, 0.15, 0.30]\n",
        "# Number of test sentences to use for code switching (per language)\n",
        "CS_N_TEST_SENTENCES = 500\n",
        "BATCH_SIZE_MT = 32\n",
        "\n",
        "# Training Size Configs\n",
        "TRAINING_SIZE_FRACS = [0.1, 0.5, 1.0]\n",
        "\n",
        "# Number of code switching error examples to return\n",
        "N_ERROR_EXAMPLES = 10\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def clean(text):\n",
        "    \"\"\"Cleans input text by removing URLs, emails, unnecessary symbols, and normalizes whitespace.\"\"\"\n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Keep only relevant characters:\n",
        "    # Cyrillic (including Ky/Kk specific), Latin, digits, space, and basic punctuation .,-?!\n",
        "    allowed_chars = r'[^а-яА-ЯёЁәіңғүұқөһӘІҢҒҮҰҚӨҺa-zA-Z0-9\\s.,?!-]'\n",
        "    text = re.sub(allowed_chars, '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def load_data(languages, n_samples, test_size, random_state):\n",
        "    \"\"\"Loads datasets from Hugging Face.\n",
        "    Perfroms cleaning, filtering, splitting and label encoding.\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "    all_labels = []\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    for lang in languages:\n",
        "        print(f\"Loading '{lang}' dataset...\")\n",
        "        try:\n",
        "            # Streaming mode enabled for efficiency\n",
        "            dataset = load_dataset(\"oscar-corpus/OSCAR-2201\",\n",
        "                                   lang,\n",
        "                                   split='train', streaming=True,\n",
        "                                   token=hf_token, trust_remote_code=True)\n",
        "            lang_samples = []\n",
        "            target_samples = n_samples[lang]\n",
        "            # Fetch more initially\n",
        "            fetch_target = int(target_samples * 1.5)\n",
        "            # islice is used to limit samples\n",
        "            iterator = tqdm(islice(dataset, fetch_target),\n",
        "                            desc=f\"Fetching & cleaning {lang} samples\", total=target_samples, unit=\" examples\")\n",
        "            count = 0\n",
        "            # Looping through dataset\n",
        "            for example in iterator:\n",
        "                # Stop when desired number of samples is reached\n",
        "                if count >= target_samples:\n",
        "                  break\n",
        "                text = example.get('text')\n",
        "                # Clean extracted text\n",
        "                if text and isinstance(text, str):\n",
        "                    cleaned = clean(text)\n",
        "                    # Check for minimum number of words\n",
        "                    if cleaned and len(cleaned.split()) >= 5:\n",
        "                        lang_samples.append(cleaned)\n",
        "                        count += 1\n",
        "                        iterator.update(1)\n",
        "            iterator.close()\n",
        "            # Handling cases when not enough sentences extracted\n",
        "            if count < target_samples:\n",
        "                print(f\"Loaded {count} valid sentences for '{lang}' (target {target_samples}).\")\n",
        "            # Adding samples to the main lists\n",
        "            texts = lang_samples[:target_samples]\n",
        "            labels = [lang] * len(texts)\n",
        "            all_texts.extend(texts)\n",
        "            all_labels.extend(labels)\n",
        "            print(f\"Loaded {len(texts)} samples for '{lang}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data for language '{lang}': {e}.\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Total samples loaded: {len(all_texts)}\")\n",
        "    print(\"Label distribution:\", Counter(all_labels))\n",
        "\n",
        "    # Convert string labels into numerical\n",
        "    le = LabelEncoder()\n",
        "    # Fit on sorted unique labels\n",
        "    unique_labels = sorted(list(set(all_labels)))\n",
        "    le.fit(unique_labels)\n",
        "    all_labels_encoded = le.transform(all_labels)\n",
        "\n",
        "    # Splitting data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        all_texts, all_labels_encoded,\n",
        "        test_size=test_size, random_state=random_state, stratify=all_labels_encoded\n",
        "    )\n",
        "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, le\n",
        "\n",
        "\n",
        "def train_tokenizer(texts, vocab_size, min_frequency, output_dir, prefix):\n",
        "    \"\"\"\n",
        "    Trains a ByteLevelBPETokenizer on the full training set.\n",
        "    Saves the tokenizer files (vocab and merges).\n",
        "    \"\"\"\n",
        "    # Create directory to store files if not exists\n",
        "    if not os.path.exists(output_dir):\n",
        "      os.makedirs(output_dir)\n",
        "    # Writing training data to a temporary file\n",
        "    temp_train_data = os.path.join(output_dir, f\"temp_train_data.txt\")\n",
        "    with open(temp_train_data, \"w\", encoding=\"utf-8\") as f:\n",
        "        for text in tqdm(texts, desc=\"Writing train data\"):\n",
        "            f.write(text + \"\\n\")\n",
        "\n",
        "    print(f\"\\nTraining Subword Tokenizer...\")\n",
        "    tokenizer = ByteLevelBPETokenizer()\n",
        "    start_time = time.time()\n",
        "    # Training BPE model based on temporary file created\n",
        "    tokenizer.train(files=[temp_train_data], vocab_size=vocab_size, min_frequency=min_frequency,\n",
        "                    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Subword Tokenizer training complete ({train_time:.2f} seconds).\")\n",
        "    tokenizer.save_model(output_dir, prefix)\n",
        "    # Delete temporary file\n",
        "    os.remove(temp_train_data)\n",
        "    # Paths to saved files\n",
        "    vocab = os.path.join(output_dir, f\"{prefix}-vocab.json\")\n",
        "    merges = os.path.join(output_dir, f\"{prefix}-merges.txt\")\n",
        "    return vocab, merges\n",
        "\n",
        "def load_tokenizer(vocab, merges):\n",
        "    \"\"\"Loads a pre-trained ByteLevelBPETokenizer from saved files.\"\"\"\n",
        "    # Checking if files exist\n",
        "    if not (os.path.exists(vocab) and os.path.exists(merges)):\n",
        "        raise FileNotFoundError(f\"Tokenizer files not found: {vocab}, {merges}\")\n",
        "    # Loading tokenizer from saved files\n",
        "    tokenizer = ByteLevelBPETokenizer(vocab, merges)\n",
        "    # Adding standard BERT-style processing\n",
        "    tokenizer.post_processor = BertProcessing(\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")), (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "    )\n",
        "    return tokenizer\n",
        "\n",
        "def get_subword_features(texts, tokenizer, vectorizer=None, max_features=50000):\n",
        "    \"\"\"\n",
        "    Tokenizes texts using the provided BPE tokenizer and then computes\n",
        "    TF-IDF features on the resulting subword sequences.\n",
        "    \"\"\"\n",
        "    print(\"Generating subword features using the tokenizer...\")\n",
        "    # Encoding each text into subword tokens\n",
        "    tokenized_texts = [\" \".join(tokenizer.encode(text).tokens) for text in tqdm(texts, desc=\"Tokenizing (Subword)\")]\n",
        "    # If tokenizer is not specified, fit a new one\n",
        "    if vectorizer is None:\n",
        "        print(\"Fitting TF-IDF Vectorizer for subwords...\")\n",
        "        vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=max_features)\n",
        "        features = vectorizer.fit_transform(tokenized_texts)\n",
        "        print(f\"Subword TF-IDF fitted: {features.shape}\")\n",
        "    else:\n",
        "        # If already fitted, transform the new texts\n",
        "        print(\"Transforming with existing subword TF-IDF Vectorizer...\")\n",
        "        features = vectorizer.transform(tokenized_texts)\n",
        "    return features, vectorizer\n",
        "\n",
        "def get_embedding_features(texts, model_name, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generates sentence embeddings using a pre-trained transformer model.\n",
        "    Uses mean pooling of the last hidden state.\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading pre-trained embedding model: {model_name}\")\n",
        "    # Loading tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    # Choose and move model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Set to evaluation mode\n",
        "    model.to(device).eval()\n",
        "    embeddings_list = []\n",
        "    # Processing texts in batches\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding Batches\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            # Tokenizing the batch\n",
        "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            # Mean pooling from the last hidden state\n",
        "            # to get a single fixed size vector representing the sentence.\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().float().numpy()\n",
        "            embeddings_list.append(embeddings)\n",
        "    # Clean up memory\n",
        "    del model, tokenizer, inputs, outputs; gc.collect();\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "    # Combining embeddings from all batches\n",
        "    return np.vstack(embeddings_list)\n",
        "\n",
        "def generate_mix_text(base_lang_text, lang_base_label, lang_mix_label, n_sentences, swap_prob, mt_model, lang_map, batch_size=16):\n",
        "    \"\"\"\n",
        "    Generates synthetic code-switched sentences using the NLLB translation model.\n",
        "    Randomly selects words with swaping probability and translates\n",
        "    them into another language (mixed).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Generating mixed data using NLLB ({swap_prob=:.2f}): {lang_base_label} -> {lang_mix_label} ---\")\n",
        "    mixed_text = []\n",
        "    original_lang = []\n",
        "    # Getting language codes for model\n",
        "    src_lang = lang_map[lang_base_label]\n",
        "    tgt_lang = lang_map[lang_mix_label]\n",
        "\n",
        "    translator, model, tokenizer = None, None, None\n",
        "    try:\n",
        "        print(f\"Loading MT model: {mt_model}\")\n",
        "        device_idx = 0 if torch.cuda.is_available() else -1\n",
        "        # Get tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(mt_model, src_lang=src_lang)\n",
        "        # Load model\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(mt_model).eval()\n",
        "        # Create translation pipeline\n",
        "        translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=src_lang, tgt_lang=tgt_lang, device=device_idx)\n",
        "        print(\"Model and pipeline loaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Clean up\n",
        "        del model, tokenizer, translator; gc.collect();\n",
        "        if torch.cuda.is_available():\n",
        "          torch.cuda.empty_cache(); return [], []\n",
        "\n",
        "    generated_count = 0\n",
        "\n",
        "    print(f\"Processing {min(len(base_lang_text), n_sentences)} base sentences ({lang_base_label})...\")\n",
        "    # Randomly select subset for translation\n",
        "    num_base_sentences = min(len(base_lang_text), n_sentences)\n",
        "    sentence_idx = random.sample(range(len(base_lang_text)), num_base_sentences)\n",
        "    iterator = tqdm(range(0, num_base_sentences, batch_size), desc=f\"Generating {lang_base_label}/{lang_mix_label} mixed set (P={swap_prob:.2f})\", unit=\"batch\")\n",
        "\n",
        "    # Process sentences in batches\n",
        "    for i in iterator:\n",
        "\n",
        "        batch_idx = range(i, min(i + batch_size, num_base_sentences))\n",
        "        actual_idx = [sentence_idx[j] for j in batch_idx]\n",
        "        batch_sentences = [base_lang_text[idx] for idx in actual_idx]\n",
        "\n",
        "        # Prepare words for translation\n",
        "        words_to_translate, sentence_struct, idx_for_flat_list = [], [], []\n",
        "        flat_word_idx_start = 0\n",
        "\n",
        "        for sent_idx, sentence_base in enumerate(batch_sentences):\n",
        "            # Whitespace split\n",
        "            tokens_base = sentence_base.split()\n",
        "            if not tokens_base:\n",
        "              sentence_struct.append({\"tokens\": [], \"swap_idx\": {}}); continue\n",
        "\n",
        "            words_to_swap, swap_idx_map = [], {}\n",
        "\n",
        "            for idx, token in enumerate(tokens_base):\n",
        "                # If token looks like a word and passes swap probability check\n",
        "                if re.search(r'\\w', token) and random.random() < swap_prob:\n",
        "                  cleaned_token = token.strip()\n",
        "                  # If not empty string\n",
        "                  if cleaned_token:\n",
        "                    swap_idx_map[idx] = len(words_to_swap)\n",
        "                    words_to_swap.append(cleaned_token)\n",
        "\n",
        "            # Store data needed for swaping\n",
        "            words_to_translate.extend(words_to_swap)\n",
        "            sentence_struct.append({\"tokens\": tokens_base, \"swap_idx\": swap_idx_map})\n",
        "            idx_for_flat_list.append((sent_idx, flat_word_idx_start, len(words_to_swap)))\n",
        "            flat_word_idx_start += len(words_to_swap)\n",
        "\n",
        "        translated_words_flat = []\n",
        "        # Translating selected words\n",
        "        if words_to_translate:\n",
        "            try:\n",
        "                # Call the pipeline to translate\n",
        "                translated_results = translator(words_to_translate, batch_size=batch_size)\n",
        "                translated_words_flat = [item['translation_text'].strip() for item in translated_results]\n",
        "            except Exception as e: print(f\"\\nWarning: Translation failed for batch: {e}\")\n",
        "\n",
        "        current_flat_idx = 0\n",
        "        # Swaping translated words\n",
        "        for sent_idx, _, n_words in idx_for_flat_list:\n",
        "            if n_words == 0:\n",
        "              continue\n",
        "\n",
        "            # Get the translated words corresponding to this sentence\n",
        "            current_translated_words = translated_words_flat[current_flat_idx : current_flat_idx + n_words]\n",
        "            current_flat_idx += n_words\n",
        "\n",
        "            if len(current_translated_words) != n_words:\n",
        "                print(f\"\\nWarning: Translation count mismatch for sentence {sent_idx} ({len(current_translated_words)} vs {n_words} expected). Using original.\")\n",
        "                continue\n",
        "\n",
        "            original_tokens = sentence_struct[sent_idx][\"tokens\"]\n",
        "            swap_idx_map = sentence_struct[sent_idx][\"swap_idx\"]\n",
        "            final_tokens = list(original_tokens) # Start with original\n",
        "            translated_word_sent_idx = 0\n",
        "            for orig_idx, token in enumerate(original_tokens):\n",
        "              if translated_word_sent_idx < len(current_translated_words):\n",
        "                # If this token was chosen for swaping\n",
        "                if orig_idx in swap_idx_map:\n",
        "                    translated_token = current_translated_words[translated_word_sent_idx]\n",
        "                    translated_word_sent_idx += 1\n",
        "                    # Replace only if the translation is non-empty and different from original\n",
        "                    if translated_token and translated_token.lower() != token.strip().lower():\n",
        "                      final_tokens[orig_idx] = translated_token # Replace in place\n",
        "\n",
        "            # Update the sentence with the new tokens\n",
        "            sentence_struct[sent_idx][\"tokens\"] = final_tokens\n",
        "\n",
        "        # Collect final sentences\n",
        "        original_label = [k for k, v in NLLB_LANG_LABELS.items() if v == src_lang][0]\n",
        "        for structure in sentence_struct:\n",
        "            final_sentence = \" \".join(structure[\"tokens\"])\n",
        "            if final_sentence:\n",
        "                mixed_text.append(final_sentence)\n",
        "                # Append the original base language label\n",
        "                original_lang.append(original_label)\n",
        "                generated_count += 1\n",
        "\n",
        "    print(f\"Finished generating {generated_count} mixed sentences for {lang_base_label} -> {lang_mix_label} (P={swap_prob:.2f}).\")\n",
        "    # Clean up memory\n",
        "    del translator, model, tokenizer; gc.collect();\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    return mixed_text, original_lang\n",
        "\n",
        "# --- Training & Evaluation ---\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, label_encoder, method, fraction, plot_cm=False):\n",
        "    \"\"\"\n",
        "    Trains a Logistic Regression model on the provided train set,\n",
        "    evaluates it on the test set, calculates metrics, and optionally\n",
        "    plots a confusion matrix.\n",
        "    \"\"\"\n",
        "    print(f\"Training & Evaluating: {method} (Fraction: {fraction:.2f})\")\n",
        "    start_time = time.time()\n",
        "    model = LogisticRegression(max_iter=1500, random_state=RANDOM_STATE, C=1.0, solver='liblinear', class_weight='balanced')\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training complete ({train_time:.2f} seconds).\")\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    eval_time = time.time() - start_time\n",
        "    print(f\"Evaluation complete ({eval_time:.2f} seconds).\")\n",
        "\n",
        "    # Calculate standard metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    metrics_dict = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'train_time': train_time,\n",
        "        'eval_time': eval_time\n",
        "    }\n",
        "    # Calculate specific F1 scores for 'ky' and 'kk'\n",
        "    for lang in ['ky', 'kk']:\n",
        "        if lang in label_encoder.classes_:\n",
        "            lang_encoded = label_encoder.transform([lang])[0]\n",
        "            metrics_dict[f'f1_{lang}'] = f1_score(y_test, y_pred, labels=[lang_encoded], average='micro')\n",
        "\n",
        "    print(f\"Evaluation for Fraction: {fraction:.2f}, Method: {method}):\")\n",
        "    # Generate report using only the labels actually present in this pair\n",
        "    labels_n = np.unique(np.concatenate((y_test, y_pred)))\n",
        "    target_names = label_encoder.inverse_transform(labels_n)\n",
        "    print(classification_report(y_test, y_pred, labels=labels_n, target_names=target_names, digits=4))\n",
        "\n",
        "    # Only plot confusion matrix for the full dataset\n",
        "    if plot_cm:\n",
        "        print(\"Generating Confusion Matrix...\")\n",
        "        try:\n",
        "            cm_labels = label_encoder.transform(label_encoder.classes_)\n",
        "            cm = confusion_matrix(y_test, y_pred, labels=cm_labels)\n",
        "            display_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            display_cm.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='vertical')\n",
        "            plt.title(f\"Confusion Matrix: {method} (Full Dataset)\\nStandard Test\")\n",
        "            plt.tight_layout()\n",
        "            filename = f\"confusion_matrix_{method.replace(' ', '_')}_standard_test.png\"\n",
        "            plt.savefig(filename)\n",
        "            plt.close(fig)\n",
        "        except Exception as e: print(f\"Could not generate plot: {e}\")\n",
        "    # Return the trained model and performance metrics\n",
        "    return model, metrics_dict\n",
        "\n",
        "# --- Plotting Function for Scaling ---\n",
        "def plot_scaling_results(results):\n",
        "    \"\"\"Plots performance metrics vs. training data fraction.\"\"\"\n",
        "    print(\"Plotting Performance Scaling\")\n",
        "    try:\n",
        "        metrics = [col for col in ['accuracy', 'f1_macro', 'f1_ky', 'f1_kk'] if col in results.columns]\n",
        "        n_metrics = len(metrics)\n",
        "        fig, axes = plt.subplots(n_metrics, 1, figsize=(10, 5 * n_metrics), sharex=True, squeeze=False)\n",
        "        axes = axes.flatten()\n",
        "        fig.suptitle(\"Performance vs. Training Data Fraction\", y=1.02)\n",
        "\n",
        "        # Plot each metric on a separate subplot\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            sns.lineplot(data=results, x='fraction', y=metric, hue='method', marker='o', ax=ax)\n",
        "            ax.set_title(f\"{metric.replace('_', ' ').title()} Scaling\")\n",
        "            ax.legend(title='Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "        axes[-1].set_xlabel(\"Training Data Fraction\")\n",
        "        # Adjust layout to prevent labels/legend overlapping\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 0.98])\n",
        "        filename = \"performance_scaling.png\"\n",
        "        plt.savefig(filename, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting scaling results: {e}\")\n",
        "\n",
        "# --- Error Analysis ---\n",
        "def display_error_examples(X_mixed, y_true_labels, y_pred_labels, n_examples=10):\n",
        "    \"\"\"\n",
        "    Prints examples of misclassified samples.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Displaying up to {n_examples} misclassified examples:\")\n",
        "    indices = list(range(len(X_mixed)))\n",
        "    random.shuffle(indices) # Show random errors\n",
        "    for i in indices:\n",
        "        # Check if there is a misclassification\n",
        "        if y_true_labels[i] != y_pred_labels[i]:\n",
        "            print(f\"Example {count + 1}:\")\n",
        "            print(f\"Original Base Lang: {y_true_labels[i]}\")\n",
        "            print(f\"Predicted Lang: {y_pred_labels[i]}\")\n",
        "            # Truncate long text examples\n",
        "            display_text = X_mixed[i][:500] + '...' if len(X_mixed[i]) > 500 else X_mixed[i]\n",
        "            print(f\"Mixed Text:\\n  '{display_text}'\")\n",
        "            print(\"-\" * 20)\n",
        "            count += 1\n",
        "            if count >= n_examples:\n",
        "                break\n",
        "    if count == 0:\n",
        "        print(\"No examples found where base language was misclassified.\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsnZIW4wZ5OG"
      },
      "outputs": [],
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Load Data\n",
        "    X_train, X_test, y_train, y_test, label_encoder = load_data(\n",
        "        LANGUAGES, N_SAMPLES, TEST_SIZE, RANDOM_STATE\n",
        "    )\n",
        "    LANG_CLASSES = label_encoder.classes_\n",
        "\n",
        "    # Train Subword Tokenizer\n",
        "    bpe_tokenizer = None\n",
        "    try:\n",
        "      vocab, merges = train_tokenizer(\n",
        "          X_train, VOCAB_SIZE, MIN_FREQUENCY, SUBWORD_MODEL_DIR, SUBWORD_MODEL_PREFIX\n",
        "      )\n",
        "      bpe_tokenizer = load_tokenizer(vocab, merges)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Failed to train or load subword tokenizer: {e}\")\n",
        "\n",
        "    all_results = []\n",
        "    final_models = {}\n",
        "    final_vectorizers = {}\n",
        "    X_test_embed = None\n",
        "\n",
        "    # Pre-generate Test Embeddings\n",
        "    print(\"\\nPre-generating embeddings for the test set...\")\n",
        "    X_test_embed = get_embedding_features(X_test, EMBEDDING_MODEL_NAME, batch_size=EMBEDDING_BATCH_SIZE)\n",
        "    if X_test_embed is not None:\n",
        "      print(f\"Test embeddings generated: {X_test_embed.shape}\")\n",
        "    else:\n",
        "      print(\"Warning: Test embedding pre-generation failed.\")\n",
        "\n",
        "    # Scaling Analysis\n",
        "    for frac in TRAINING_SIZE_FRACS:\n",
        "        print(f\"Processing Training Fraction: {frac:.2f}\")\n",
        "\n",
        "        # Create Training Subset\n",
        "        if frac < 1.0:\n",
        "            X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "                X_train, y_train,\n",
        "                train_size=frac,\n",
        "                random_state=RANDOM_STATE,\n",
        "                stratify=y_train\n",
        "            )\n",
        "        else:\n",
        "            X_train_sub, y_train_sub = X_train, y_train\n",
        "        print(f\"Using {len(X_train_sub)} training samples.\")\n",
        "\n",
        "        # Plot confusion matrix only for the full dataset\n",
        "        plot_cm = (frac == 1.0)\n",
        "\n",
        "        # Train and Evaluate Models on Subset\n",
        "\n",
        "        # Method 1: Character N-grams\n",
        "        method_name_char = \"Character N-grams\"\n",
        "        try:\n",
        "            print(f\"\\nStarting {method_name_char} for fraction {frac:.2f}...\")\n",
        "            vectorizer_char = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=50000)\n",
        "            print(\"Fitting Char TF-IDF...\")\n",
        "            X_train_feat_char = vectorizer_char.fit_transform(X_train_sub)\n",
        "            X_test_feat_char = vectorizer_char.transform(X_test)\n",
        "            # Train model, evaluate, get metrics\n",
        "            model_char, metrics_char = train_and_evaluate(\n",
        "                X_train_feat_char, y_train_sub, X_test_feat_char, y_test, label_encoder,\n",
        "                method_name_char, frac, plot_cm=plot_cm\n",
        "            )\n",
        "            metrics_char['fraction'] = frac\n",
        "            metrics_char['method'] = method_name_char\n",
        "            all_results.append(metrics_char)\n",
        "            if plot_cm:\n",
        "                # Save final model and vectorizer\n",
        "                final_models[method_name_char] = model_char\n",
        "                final_vectorizers[method_name_char] = vectorizer_char\n",
        "            del X_train_feat_char, X_test_feat_char, model_char; gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during {method_name_char} processing (Fraction {frac:.2f}): {e}\")\n",
        "\n",
        "        # Method 2: Subword Units (BPE + TF-IDF)\n",
        "        method_name_subword = \"Subword Units\"\n",
        "        if bpe_tokenizer:\n",
        "            try:\n",
        "                print(f\"\\nStarting {method_name_subword} for fraction {frac:.2f}...\")\n",
        "                # Generate subword TF-IDF features\n",
        "                X_train_feat_sub, vectorizer_sub = get_subword_features(\n",
        "                    X_train_sub, bpe_tokenizer, vectorizer=None, max_features=50000\n",
        "                )\n",
        "                # Transform the test set using the fitted vectorizer\n",
        "                X_test_feat_sub, _ = get_subword_features(\n",
        "                    X_test, bpe_tokenizer, vectorizer=vectorizer_sub\n",
        "                )\n",
        "                # Train model, evaluate, get metrics\n",
        "                model_sub, metrics_sub = train_and_evaluate(\n",
        "                    X_train_feat_sub, y_train_sub, X_test_feat_sub, y_test, label_encoder,\n",
        "                    method_name_subword, frac, plot_cm=plot_cm\n",
        "                )\n",
        "                metrics_sub['fraction'] = frac\n",
        "                metrics_sub['method'] = method_name_subword\n",
        "                all_results.append(metrics_sub)\n",
        "                if plot_cm:\n",
        "                    final_models[method_name_subword] = model_sub\n",
        "                    final_vectorizers[method_name_subword] = vectorizer_sub\n",
        "                del X_train_feat_sub, X_test_feat_sub, model_sub; gc.collect()\n",
        "            except Exception as e:\n",
        "                print(f\"Error during {method_name_subword} processing (Fraction {frac:.2f}): {e}\")\n",
        "\n",
        "\n",
        "        # Method 3: Embeddings\n",
        "        method_name_embed = \"Embeddings\"\n",
        "        try:\n",
        "            print(f\"\\nStarting {method_name_embed} (Fraction {frac:.2f})...\")\n",
        "            # Generate embeddings specifically for the current training subset\n",
        "            X_train_feat_embed = get_embedding_features(X_train_sub, EMBEDDING_MODEL_NAME, batch_size=EMBEDDING_BATCH_SIZE)\n",
        "            if X_train_feat_embed is not None and X_test_embed is not None: # Check both train and pre-generated test embeddings\n",
        "                # Train model, evaluate, get metrics using pre-generated test embeddings\n",
        "                model_embed, metrics_embed = train_and_evaluate(\n",
        "                    X_train_feat_embed, y_train_sub, X_test_embed, y_test, label_encoder, method_name_embed, frac, plot_cm=plot_cm)\n",
        "                if model_embed:\n",
        "                     metrics_embed['fraction'] = frac\n",
        "                     metrics_embed['method'] = method_name_embed\n",
        "                     all_results.append(metrics_embed)\n",
        "                     if plot_cm:\n",
        "                      final_models[method_name_embed] = model_embed\n",
        "                      final_vectorizers[method_name_embed] = None # No vectorizer\n",
        "            elif X_test_embed is None:\n",
        "              print(\"Skipping evaluation as test embeddings failed pre-generation.\")\n",
        "            # Clean train embeddings\n",
        "            del X_train_feat_embed; gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "        except Exception as e:\n",
        "            print(f\"Error during {method_name_embed}: {e}\")\n",
        "            if torch.cuda.is_available():\n",
        "              torch.cuda.empty_cache()\n",
        "\n",
        "    # End of Fraction Loop\n",
        "\n",
        "    # Plot Scaling Results\n",
        "    if all_results:\n",
        "        results = pd.DataFrame(all_results)\n",
        "        print(\"\\nOverall Performance Metrics Across Fractions\")\n",
        "        display_cols = [col for col in ['method', 'fraction', 'accuracy', 'f1_macro', 'f1_ky', 'f1_kk', 'train_time', 'eval_time'] if col in results.columns]\n",
        "        print(results[display_cols].round(4))\n",
        "        results.to_csv(\"scaling_results.csv\", index=False)\n",
        "        plot_scaling_results(results)\n",
        "    else:\n",
        "      print(\"\\nNo scaling results collected.\")\n",
        "\n",
        "    # Code-Switching Evaluation\n",
        "\n",
        "    print(f\"\\n Code-Switching Evaluation\")\n",
        "    if not final_models:\n",
        "      print(\"No final models trained.\")\n",
        "    else:\n",
        "        # Prepare a dictionary mapping language labels to their corresponding test sentences\n",
        "        y_test_labels_str = label_encoder.inverse_transform(y_test)\n",
        "        test_sentences = {lang: [X_test[i] for i, l in enumerate(y_test_labels_str) if l == lang] for lang in LANG_CLASSES}\n",
        "\n",
        "        # Iterate through the specified code-switching probabilities\n",
        "        for swap_prob in WORD_SWAP_PROBS:\n",
        "            print(f\"\\nProcessing Swap Probability: {swap_prob:.2f}\")\n",
        "            X_mixed, original_langs_mixed = [], []\n",
        "            for base_lang in BASE_LANGS:\n",
        "                if base_lang not in NLLB_LANG_LABELS:\n",
        "                  continue\n",
        "                # Generate sentences with specified swap probability\n",
        "                mixed_subset, original_subset = generate_mix_text(\n",
        "                    test_sentences.get(base_lang, []),\n",
        "                    base_lang,\n",
        "                    MIX_LANG,\n",
        "                    CS_N_TEST_SENTENCES,\n",
        "                    swap_prob,\n",
        "                    NLLB_MODEL,\n",
        "                    NLLB_LANG_LABELS,\n",
        "                    BATCH_SIZE_MT)\n",
        "                X_mixed.extend(mixed_subset)\n",
        "                original_langs_mixed.extend(original_subset)\n",
        "                gc.collect();\n",
        "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "            if not X_mixed:\n",
        "              print(f\"No mixed sentences for P={swap_prob:.2f}.\")\n",
        "              continue\n",
        "\n",
        "            print(f\"\\nEvaluating on {len(X_mixed)} Mixed Sentences (P={swap_prob:.2f}) ---\")\n",
        "            y_true_mixed = label_encoder.transform(original_langs_mixed)\n",
        "\n",
        "            # Evaluate each final model (trained on full dataset) on this mixed data\n",
        "            for method, model in final_models.items():\n",
        "                print(f\"\\nEvaluating {method} (P={swap_prob:.2f})...\")\n",
        "                vectorizer = final_vectorizers.get(method)\n",
        "                X_mixed_feat = None\n",
        "                try:\n",
        "                    # Get Features for the Mixed Data\n",
        "                    if method == \"Character N-grams\":\n",
        "                        if vectorizer:\n",
        "                          X_mixed_feat = vectorizer.transform(X_mixed)\n",
        "                    elif method == \"Subword Units\":\n",
        "                        if vectorizer and bpe_tokenizer:\n",
        "                          X_mixed_feat, _ = get_subword_features(X_mixed, bpe_tokenizer, vectorizer=vectorizer)\n",
        "                    elif method == \"Embeddings\":\n",
        "                        X_mixed_feat = get_embedding_features(X_mixed, EMBEDDING_MODEL_NAME, batch_size=EMBEDDING_BATCH_SIZE)\n",
        "\n",
        "                    if X_mixed_feat is None:\n",
        "                      print(f\"Feature generation failed for {method}.\")\n",
        "                      continue\n",
        "\n",
        "                    # Predict & Evaluate\n",
        "                    y_pred_mixed_encoded = model.predict(X_mixed_feat)\n",
        "                    y_pred_mixed_labels = label_encoder.inverse_transform(y_pred_mixed_encoded)\n",
        "\n",
        "                    # Report the distribution of predicted labels\n",
        "                    print(f\"Predicted label distribution: {Counter(y_pred_mixed_labels)}\")\n",
        "                    if MIX_LANG not in y_pred_mixed_labels:\n",
        "                      print(f\"Warning: Mixed lang '{MIX_LANG}' was not predicted.\")\n",
        "\n",
        "                    # Print the detailed classification report\n",
        "                    print(f\"\\nClassification Report ({method} Mix P={swap_prob:.2f}):\")\n",
        "                    labels_n = np.unique(np.concatenate((y_true_mixed, y_pred_mixed_encoded)))\n",
        "                    target_names = label_encoder.inverse_transform(labels_n)\n",
        "                    print(classification_report(y_true_mixed, y_pred_mixed_encoded, labels=labels_n, target_names=target_names, digits=4, zero_division=0))\n",
        "\n",
        "                    # Generate and save the confusion matrix for mixed results\n",
        "                    print(f\"Confusion Matrix ({method} Mix P={swap_prob:.2f}):\")\n",
        "                    cm_str = sorted(list(set(BASE_LANGS + [MIX_LANG])))\n",
        "                    cm_filtered = [l for l in cm_str if l in label_encoder.classes_]\n",
        "                    cm_encoded = label_encoder.transform(cm_filtered)\n",
        "                    cm_mixed = confusion_matrix(y_true_mixed, y_pred_mixed_encoded, labels=cm_encoded)\n",
        "                    display_mixed = ConfusionMatrixDisplay(confusion_matrix=cm_mixed, display_labels=cm_filtered)\n",
        "                    fig_mix, ax_mix = plt.subplots(figsize=(8, 6))\n",
        "                    display_mixed.plot(cmap=plt.cm.Blues, ax=ax_mix, xticks_rotation='vertical')\n",
        "                    plt.title(f\"Confusion Matrix: {method} Mix P={swap_prob:.2f}\")\n",
        "                    plt.tight_layout()\n",
        "                    filename_mix = f\"confusion_matrix_{method.replace(' ', '_')}_mix_p{swap_prob:.2f}.png\"\n",
        "                    plt.savefig(filename_mix)\n",
        "                    plt.close(fig_mix)\n",
        "\n",
        "                    # Display Error Examples\n",
        "                    display_error_examples(X_mixed, original_langs_mixed, y_pred_mixed_labels, n_examples=N_ERROR_EXAMPLES)\n",
        "                    #Clean up\n",
        "                    del X_mixed_feat; gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                      torch.cuda.empty_cache()\n",
        "\n",
        "                except Exception as e: print(f\"\\nERROR during {method} NLLB evaluation (P={swap_prob:.2f}): {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2naPJBm5mtAX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}